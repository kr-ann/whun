{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PEGASUS_title_jokes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPs6a5R3A2Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e59a503-4a0e-43cf-ccb6-7d5396e47a73"
      },
      "source": [
        "!pip install transformers\n",
        "!pip3 install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 18.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 957 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 35.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.3\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vcfeB4oTec_",
        "outputId": "280869bb-a4e9-4b9f-8833-71dae6211170"
      },
      "source": [
        "import datetime\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f44ff349bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2Wc0WHOBZ2O"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlK2infy7uCC"
      },
      "source": [
        "# Load model checkpoint from huggingface Library\n",
        "\n",
        "Load the model which you want to use and load the tokenizer for that model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDDGcVdTpP0"
      },
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name = 'google/pegasus-xsum'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcpC4COv7pdb"
      },
      "source": [
        "# Download and Prepare Data\n",
        "\n",
        "Download the data from github repo. Load the dataset from the .json file and remove the unwanted columns. Divide the dataset for training and validation. Use the categories in validation data to generate jokes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VplF9g3HT3iY"
      },
      "source": [
        "!git clone https://github.com/taivop/joke-dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esl4nGNNWVn2"
      },
      "source": [
        "y = pd.read_json('joke-dataset/wocka.json')\n",
        "del y['id']\n",
        "del y['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRe6m4c5T4ZB"
      },
      "source": [
        "z = pd.read_json('joke-dataset/stupidstuff.json')\n",
        "del z['id']\n",
        "del z['score'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjU7jzEKCaYg"
      },
      "source": [
        "sum_data = pd.concat([y,z])\n",
        "sum_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6wdeci-T6tD"
      },
      "source": [
        "sum_data = sum_data.sample(len(sum_data), random_state=20)\n",
        "train_sub = int(len(sum_data) * 0.99)\n",
        "\n",
        "train_df = sum_data[0:train_sub]\n",
        "val_df = sum_data[train_sub:]\n",
        "\n",
        "train_texts = list(train_df['title'])\n",
        "val_texts = list(val_df['title'])\n",
        "\n",
        "train_decode = list(train_df['body'])\n",
        "val_decode = list(val_df['body'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IA3N-a__Q7W"
      },
      "source": [
        "# Tokenize\n",
        "\n",
        "Tokenize the data and convert them to a pytorch data object for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OagFc2yUOg7"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, max_length=64, truncation=True, padding='longest')\n",
        "val_encodings = tokenizer(val_texts, max_length=64, truncation=True, padding='longest')\n",
        "\n",
        "train_labels = tokenizer(train_decode, max_length=256, truncation=True, padding='longest')\n",
        "val_labels = tokenizer(val_decode, max_length=256, truncation=True, padding='longest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3W6M9kpUSXl"
      },
      "source": [
        "class Summary_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5E5g8SeUUGr"
      },
      "source": [
        "train_dataset = Summary_dataset(train_encodings, train_labels)\n",
        "val_dataset = Summary_dataset(val_encodings, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FML_o5eL7kB1"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a3nh5fjUUiU"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=100,              # total number of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=5,\n",
        "    eval_accumulation_steps=1,\n",
        "    learning_rate=1e-4,\n",
        "    adafactor = True                #use adafactor instead of adam\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUZPFCX0UV36"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoFDCOsUYC-"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r0GurVX7eS5"
      },
      "source": [
        "# Generate Text\n",
        "\n",
        "Generate text using different sets of arguments. You can find more on generating text here: [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0e_foQ7dJHH"
      },
      "source": [
        "batch = tokenizer('If a train is traveling at 80 mph from Chicago to Cleveland, how may flapjacks does it take to cover the roof of a doghouse?', truncation=True, padding='longest', return_tensors=\"pt\").to(torch_device)\n",
        "generated = model.generate(**batch, min_length=64, do_sample=True, top_p=0.92, top_k=50, num_beams=8, no_repeat_ngram_size=1)\n",
        "tgt_text = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
        "tgt_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbMjvB158D67"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHcazk-RVpI5"
      },
      "source": [
        "trainer.save_model('pegasus_jokes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVBu_8mK8J3J"
      },
      "source": [
        "# Load saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84X8Jnpr8paP"
      },
      "source": [
        "from transformers import PegasusConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oGwhzix8I1L"
      },
      "source": [
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')\n",
        "config = PegasusConfig.from_json_file('./content/saved_model/*.config') #Path of .config file\n",
        "model = PegasusForConditionalGeneration.from_pretrained('', config=config).to(torch_device) #path of .bin file"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}